{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "For the final project, your task is to predict the type of galaxies based on a number of measurements and calculations about the images of the galaxies.  This is similar to the [Galaxy Zoo](http://zoo1.galaxyzoo.org/) project we discussed in class, but rather than starting from the images themselves, we are starting with some properties of the galaxies measured from the images.\n",
    "\n",
    "The data are fully described in [Fischer et al (2019)](https://arxiv.org/abs/1811.02580), a recent paper by several U. Penn researchers, including Helena Dominguez-Sanchez, whom you met in class on April 17.\n",
    "Don't worry.  You don't need to read that paper to do this project.  Helena explained the important aspects of the data in class.  (Her slides are available on [canvas](https://canvas.upenn.edu/files/76244434/).)  But if you feel inspired to read more about it, please do take a look at the paper.\n",
    "\n",
    "The Fischer et al paper describes quite a lot of measurements based on different kinds of fits to the images.  We have extracted a subset of the measurements into a smaller catalog, which should be easier for you to work with.  Specifically, we selected columns related to the bulge + disk decomposition of the galaxy (called SE for Sersic + Exponential in the paper).\n",
    "\n",
    "The columns in this catalog are:\n",
    "\n",
    "* id = A numeric ID identifying the galaxy\n",
    "* gal_type = Which type of galaxy this is.  We discuss this more below.\n",
    "* ra = Right Ascension in degrees.  Kind of like longitude on the sky.\n",
    "* dec = Declination in degrees.  Like latitude on the sky.\n",
    "* redshift = A measure of how far away the galaxy is.\n",
    "* log_luminosity = Base 10 logarithm of the galaxy's brightness (aka luminosity) measured as a multiple of the sun's luminosity.\n",
    "* color = Lr / Lg, the ratio of the luminosity at red wavelengths to the luminosity at green wavelengths.\n",
    "* radius = The half-light radius of the galaxy (called Re in Helena's slides) according to the bulge + disk fit in kiloparsecs (kpc; see below for definition of kpc).\n",
    "* b_over_a = The ratio of the semi-minor axis of the galaxy (b) to the semi-major axis (a).  b_over_a = 1 is a circle. b_over_a close to 0 is a very elongated ellipse.\n",
    "* pos_angle = The position angle of the semi-minor axis of the galaxy on the sky in degrees.\n",
    "* bulge_fract = The fraction of the total light that was found to be in the bulge component (called B/T in Helena's slides).\n",
    "* sersic_n = The Sersic index of the bulge component of the fit.\n",
    "* r_bulge = The half-light radius of the bulge component of the fit in kiloparsecs (kpc).\n",
    "* r_disk = The half-light radius of the disk component of the fit in kiloparsecs (kpc).\n",
    "\n",
    "Radii are given in kpc above, a common distance unit used by astronomers. For reference, 1 kpc = 1000 pc, and 1 pc = 3.26 light years.\n",
    "\n",
    "--- \n",
    "\n",
    "The gal_type column is your target (y).  The others you may use as your predictors (x_i).\n",
    "\n",
    "For the galaxy type, we distilled the morphological classification described in Fischer et al into 4 broad categories.\n",
    "\n",
    "* gal_type = 1 are elliptical galaxies.  In the paper, these are called E.  They have TTYPE <= 0 and P_S0 <= 0.5.\n",
    "* gal_type = 2 are lenticular galaxies.  In the paper, these are called S0.  They have TTYPE <= 0 and P_S0 > 0.5.\n",
    "* gal_type = 3 are tight spiral galaxies.  In the paper, these are described as 0 < TType < 3.  We will call them S1.\n",
    "* gal_type = 4 are loose spiral galaxies.  In the paper, these are described as TType > 3.  We will call them S2.\n",
    "\n",
    "Note: in addition to the catalog described above, we are also providing the complete catalog described in Fischer et al for g and r bands.  You are not required to use it, but you may do so if you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "For the first stage of the project, we'll start with the easier task of just distinguishing the two most extreme galaxy types: 1 vs 4, or E vs S2.\n",
    "\n",
    "You may use any of the various characterization methods we have used throughout the course to do the prediction.  (Or even ones we haven't covered if you feel so inclined.)  You should write two functions:\n",
    "\n",
    "* train_galaxy_predictions takes as input a set of training data and returns some results, which can be used to make predictions.\n",
    "* validate_galaxy_predictions takes the results from the first function and some validation data to predict the galaxy types.  It should return n array of predicted galaxy types.\n",
    "\n",
    "You can decide how you want to split the provided data into training and validation samples.  (When you submit your project, we will run your code using the full given data as the training set.  We have reserved a validation sample of approximately equal size to use for the validation step.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules and functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn.preprocessing import StandardScaler as scale\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "def train_galaxy_predictions_1(gal_type, data, data_g, data_r):\n",
    "    \"\"\"Do some kind of training to learn how to predict the galaxy type from the given training data.\n",
    "    \"\"\"\n",
    "    \n",
    "    #seed predictor data and response into dataframes for easier manipulation\n",
    "    predictor = pd.DataFrame(gal_type)    \n",
    "    df_data = pd.DataFrame(data)\n",
    "    \n",
    "    #delete unwanted columns, i.e. false predictors\n",
    "    df_data = df_data.drop(['id', 'gal_type'], axis= 1)\n",
    "\n",
    "    #convert back from dataframes to arrays since that is what functions as input in keras\n",
    "    train = df_data.values\n",
    "    train_response = predictor.values\n",
    "    \n",
    "    #since this is a binary classification problem, assign a value of 0 to one of the response possibilities (galaxy type 1)\n",
    "    #and 1 for the other (galaxy type 4). this is called one-hot encoding.\n",
    "    train_response[train_response == 1] = 0\n",
    "    train_response[train_response == 4] = 1\n",
    "   \n",
    "    #scale the predictor data accordingly.\n",
    "    #scale function applies a transformation that results in a standard normal distribution ~ Z(0,1)\n",
    "    sc = scale()\n",
    "    scale_train = sc.fit_transform(train)\n",
    "    \n",
    "    #build the tree with dropout layers to prevent overfitting\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(input_shape = (df_data.values.shape[1],), units= 10, kernel_initializer= 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dropout(0.2, noise_shape=None, seed=None))\n",
    "    classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'linear'))\n",
    "    classifier.add(Dropout(0.2, noise_shape=None, seed=None))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    \n",
    "    #compile tree with chosen optimizer and loss function\n",
    "    #binary cross-entropy is chosen as this is a binary classification problem\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    #train neural net\n",
    "    classifier.fit(scale_train, train_response, batch_size = 3, epochs = 15)\n",
    "    \n",
    "    #return neural net as an object for use\n",
    "    return(classifier)\n",
    "\n",
    "def validate_galaxy_predictions_1(result, data, data_g, data_r):\n",
    "    \"\"\"Use the output from the above function to predict the galaxy type for some other validation data.\n",
    "    \"\"\"\n",
    "    #use neural net that resulted from running train_galaxy_predictions method\n",
    "    classifier = result\n",
    "    \n",
    "    #seed predictor data into dataframe for easier manipulation\n",
    "    df_data = pd.DataFrame(data)\n",
    "    \n",
    "    #delete unwanted columns, i.e. false predictors\n",
    "    df_data = df_data.drop(['id', 'gal_type'], axis= 1)\n",
    "    \n",
    "    #convert back from dataframes to arrays since that is what functions as input in keras\n",
    "    test = df_data.values\n",
    "    \n",
    "    #scale the predictor data accordingly.\n",
    "    #scale function applies a transformation that results in a standard normal distribution ~ Z(0,1)\n",
    "    sc = scale()\n",
    "    scale_test = sc.fit_transform(test)\n",
    "    \n",
    "    #predict which class the data is in\n",
    "    y_pred = classifier.predict(scale_test)\n",
    "    \n",
    "    #assign class based on convention introduced in creation of tree.\n",
    "    y_pred[y_pred >= .5] = 4\n",
    "    y_pred[y_pred < .5] = 1\n",
    "    \n",
    "    #reformat\n",
    "    y_pred = y_pred.reshape((y_pred.shape[0],))\n",
    "    \n",
    "    #return predictor values\n",
    "    return(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3868 - acc: 0.8630\n",
      "Epoch 2/15\n",
      "1000/1000 [==============================] - 0s 354us/step - loss: 0.1305 - acc: 0.9550\n",
      "Epoch 3/15\n",
      "1000/1000 [==============================] - 0s 374us/step - loss: 0.1111 - acc: 0.9610\n",
      "Epoch 4/15\n",
      "1000/1000 [==============================] - 0s 367us/step - loss: 0.0958 - acc: 0.9660\n",
      "Epoch 5/15\n",
      "1000/1000 [==============================] - 0s 481us/step - loss: 0.0957 - acc: 0.9700\n",
      "Epoch 6/15\n",
      "1000/1000 [==============================] - 0s 477us/step - loss: 0.0934 - acc: 0.9700\n",
      "Epoch 7/15\n",
      "1000/1000 [==============================] - 0s 476us/step - loss: 0.0871 - acc: 0.9710\n",
      "Epoch 8/15\n",
      "1000/1000 [==============================] - 0s 371us/step - loss: 0.0919 - acc: 0.9680\n",
      "Epoch 9/15\n",
      "1000/1000 [==============================] - 0s 484us/step - loss: 0.0918 - acc: 0.9720\n",
      "Epoch 10/15\n",
      "1000/1000 [==============================] - 0s 374us/step - loss: 0.0909 - acc: 0.9720\n",
      "Epoch 11/15\n",
      "1000/1000 [==============================] - 0s 358us/step - loss: 0.0870 - acc: 0.9740\n",
      "Epoch 12/15\n",
      "1000/1000 [==============================] - 0s 346us/step - loss: 0.0811 - acc: 0.9760\n",
      "Epoch 13/15\n",
      "1000/1000 [==============================] - 0s 358us/step - loss: 0.0914 - acc: 0.9720\n",
      "Epoch 14/15\n",
      "1000/1000 [==============================] - 0s 345us/step - loss: 0.0858 - acc: 0.9720\n",
      "Epoch 15/15\n",
      "1000/1000 [==============================] - 0s 352us/step - loss: 0.0908 - acc: 0.9720\n",
      "Num correct =  194\n",
      "Num total =  198\n",
      "Fraction correct =  0.9797979797979798\n",
      "Test is: \n",
      "[[ 59   2]\n",
      " [  2 135]]\n",
      "Train is: \n",
      "[[366  11]\n",
      " [ 15 608]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "gal_file_name = 'training_galaxies.dat'\n",
    "full_g_file_name = 'training_galaxies_full_g.dat'\n",
    "full_r_file_name = 'training_galaxies_full_r.dat'\n",
    "\n",
    "gal_data = np.genfromtxt(gal_file_name, names=True, dtype=None)\n",
    "full_g_data = np.genfromtxt(full_g_file_name, names=True, dtype=None)\n",
    "full_r_data = np.genfromtxt(full_r_file_name, names=True, dtype=None)\n",
    "\n",
    "# For now, limit the data to those galaxies with gal_type == 1 or 4\n",
    "gal_type = gal_data['gal_type'].copy()\n",
    "type_1_4 = (gal_type == 1) | (gal_type == 4)\n",
    "gal_data = gal_data[type_1_4]\n",
    "full_g_data = full_g_data[type_1_4]\n",
    "full_r_data = full_r_data[type_1_4]\n",
    "gal_type = gal_type[type_1_4]\n",
    "\n",
    "# Zero out the gal_type column in the input data, so it can't be used to cheat.  :)\n",
    "gal_data['gal_type'][:] = 0\n",
    "\n",
    "# Split up the input into a training and validation set as you want.\n",
    "# When we run this, we will use the full data arrays as input, and some different (not provided!) data as validation.\n",
    "### EDIT THIS CODE...\n",
    "\n",
    "#generalize so you are aable to choose various fractions of available data to run through neural net\n",
    "n=1000\n",
    "training_data = gal_data[:n]\n",
    "training_data_g = full_g_data[:n]\n",
    "training_data_r = full_r_data[:n]\n",
    "training_gal_type = gal_type[:n]\n",
    "validation_data = gal_data[n:]\n",
    "validation_data_g = full_g_data[n:]\n",
    "validation_data_r = full_r_data[n:]\n",
    "validation_gal_type = gal_type[n:]\n",
    "\n",
    "# Your function can ignore training_data_g and training_data_r if you don't want to use them.\n",
    "# But the function signature needs to take 4 values, since we will pass them to your function when we run this.\n",
    "result = train_galaxy_predictions_1(training_gal_type, training_data, training_data_g, training_data_r)\n",
    "predicted_gal_type = validate_galaxy_predictions_1(result, validation_data, validation_data_g, validation_data_r)\n",
    "\n",
    "#run predict function on the training set to see how well it does (see confusion matrix for training data set below)\n",
    "predict_train = validate_galaxy_predictions_1(result, training_data, training_data_g, training_data_r)\n",
    "\n",
    "print('Num correct = ', np.sum(predicted_gal_type == validation_gal_type))\n",
    "print('Num total = ',len(validation_gal_type))\n",
    "print('Fraction correct = ',np.sum(predicted_gal_type == validation_gal_type) / len(validation_gal_type))\n",
    "# Note: It may be helpful to construct a confusion matrix to better quantify how well you are doing here, rather\n",
    "# than just looking at the overal fraction correct.\n",
    "\n",
    "#create confusion matrix for test\n",
    "confusion_test = confusion_matrix(predicted_gal_type, validation_gal_type)\n",
    "\n",
    "#create confusion matrix for train data set\n",
    "training_gal_type[training_gal_type == 1] = 4\n",
    "training_gal_type[training_gal_type == 0] = 1\n",
    "confusion_train = confusion_matrix(predict_train, training_gal_type)\n",
    "\n",
    "#print out confusion matrices\n",
    "print(\"Test is: \")\n",
    "print(confusion_test)\n",
    "print(\"Train is: \")\n",
    "print(confusion_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "The second stage of the project is the same, except we will try to classify the spirals into their two sub-categories, S1 vs S2 (i.e. types 3 vs 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules and functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler as scale\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "def train_galaxy_predictions_2(gal_type, data, data_g, data_r):\n",
    "    \"\"\"Do some kind of training to learn how to predict the galaxy type from the given training data.\n",
    "    \"\"\"\n",
    "    \n",
    "    #seed predictor data and response into dataframes for easier manipulation\n",
    "    predictor = pd.DataFrame(gal_type)    \n",
    "    df_data = pd.DataFrame(data)\n",
    "    \n",
    "    #delete unwanted columns, i.e. false predictors\n",
    "    df_data = df_data.drop(['id', 'gal_type'], axis= 1)\n",
    "    \n",
    "    #convert back from dataframes to arrays since that is what functions as input in keras\n",
    "    train = df_data.values\n",
    "    train_response = predictor.values\n",
    "    \n",
    "    #since this is a binary classification problem, assign a value of 0 to one of the response possibilities (galaxy type 3)\n",
    "    #and 1 for the other (galaxy type 4). this is called one-hot encoding.\n",
    "    train_response[train_response == 3] = 0\n",
    "    train_response[train_response == 4] = 1\n",
    "    \n",
    "    #scale the predictor data accordingly.\n",
    "    #scale function applies a transformation that results in a standard normal distribution ~ Z(0,1)\n",
    "    sc = scale()\n",
    "    scale_train = sc.fit_transform(train)\n",
    "    \n",
    "    #build the tree with dropout layers to prevent overfitting\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(input_shape = (df_data.values.shape[1],), units= 16, kernel_initializer= 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dropout(0.5, noise_shape=None, seed=None))\n",
    "    classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'linear'))\n",
    "    classifier.add(Dropout(0.5, noise_shape=None, seed=None))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    \n",
    "    #compile tree with chosen optimizer and loss function\n",
    "    #binary cross-entropy is chosen as this is a binary classification problem\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    #train neural net\n",
    "    classifier.fit(scale_train, train_response, batch_size = 3, epochs = 15)\n",
    "    \n",
    "    #return neural net as an object for use\n",
    "    return(classifier)\n",
    "\n",
    "def validate_galaxy_predictions_2(result, data, data_g, data_r):\n",
    "    \"\"\"Use the output from the above function to predict the galaxy type for some other validation data.\n",
    "    \"\"\"\n",
    "    \n",
    "    #use neural net that resulted from running train_galaxy_predictions_2 method\n",
    "    classifier = result\n",
    "    \n",
    "    #seed predictor data into dataframe for easier manipulation\n",
    "    df_data = pd.DataFrame(data)\n",
    "    \n",
    "    #delete unwanted columns, i.e. false predictors\n",
    "    df_data = df_data.drop(['id', 'gal_type'], axis= 1)\n",
    "    \n",
    "    #convert back from dataframes to arrays since that is what functions as input in keras\n",
    "    test = df_data.values\n",
    "    \n",
    "    #scale the predictor data accordingly.\n",
    "    #scale function applies a transformation that results in a standard normal distribution ~ Z(0,1)\n",
    "    sc = scale()\n",
    "    scale_test = sc.fit_transform(test)\n",
    "    \n",
    "    #predict which class the data is in\n",
    "    y_pred = classifier.predict(scale_test)\n",
    "    \n",
    "    #assign class based on convention introduced in creation of tree.\n",
    "    y_pred[y_pred >= .5] = 4\n",
    "    y_pred[y_pred < .5] = 3\n",
    "     \n",
    "    #reformat\n",
    "    y_pred = y_pred.reshape((y_pred.shape[0],))\n",
    "    \n",
    "    #return predictor values\n",
    "    return(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6403 - acc: 0.6240\n",
      "Epoch 2/15\n",
      "1000/1000 [==============================] - 0s 446us/step - loss: 0.5040 - acc: 0.7410\n",
      "Epoch 3/15\n",
      "1000/1000 [==============================] - 0s 459us/step - loss: 0.4971 - acc: 0.7680\n",
      "Epoch 4/15\n",
      "1000/1000 [==============================] - 0s 365us/step - loss: 0.4816 - acc: 0.7870\n",
      "Epoch 5/15\n",
      "1000/1000 [==============================] - 0s 349us/step - loss: 0.4539 - acc: 0.8150\n",
      "Epoch 6/15\n",
      "1000/1000 [==============================] - 0s 348us/step - loss: 0.4745 - acc: 0.8030\n",
      "Epoch 7/15\n",
      "1000/1000 [==============================] - 0s 347us/step - loss: 0.4677 - acc: 0.8050\n",
      "Epoch 8/15\n",
      "1000/1000 [==============================] - 0s 350us/step - loss: 0.4844 - acc: 0.8050\n",
      "Epoch 9/15\n",
      "1000/1000 [==============================] - 0s 356us/step - loss: 0.4718 - acc: 0.8100\n",
      "Epoch 10/15\n",
      "1000/1000 [==============================] - 0s 347us/step - loss: 0.4777 - acc: 0.7990\n",
      "Epoch 11/15\n",
      "1000/1000 [==============================] - 0s 390us/step - loss: 0.4710 - acc: 0.8030\n",
      "Epoch 12/15\n",
      "1000/1000 [==============================] - 0s 447us/step - loss: 0.4621 - acc: 0.8080\n",
      "Epoch 13/15\n",
      "1000/1000 [==============================] - 0s 489us/step - loss: 0.4415 - acc: 0.8080\n",
      "Epoch 14/15\n",
      "1000/1000 [==============================] - 0s 363us/step - loss: 0.4678 - acc: 0.8020\n",
      "Epoch 15/15\n",
      "1000/1000 [==============================] - 0s 357us/step - loss: 0.4510 - acc: 0.8260\n",
      "Num correct =  146\n",
      "Num total =  179\n",
      "Fraction correct =  0.8156424581005587\n",
      "Test is: \n",
      "[[ 39  21]\n",
      " [ 12 107]]\n",
      "Train is: \n",
      "[[282  84]\n",
      " [ 90 544]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "gal_file_name = 'training_galaxies.dat'\n",
    "full_g_file_name = 'training_galaxies_full_g.dat'\n",
    "full_r_file_name = 'training_galaxies_full_r.dat'\n",
    "\n",
    "gal_data = np.genfromtxt(gal_file_name, names=True, dtype=None)\n",
    "full_g_data = np.genfromtxt(full_g_file_name, names=True, dtype=None)\n",
    "full_r_data = np.genfromtxt(full_r_file_name, names=True, dtype=None)\n",
    "\n",
    "# This time, limit the data to those galaxies with gal_type == 3 or 4\n",
    "gal_type = gal_data['gal_type'].copy()\n",
    "type_3_4 = (gal_type == 3) | (gal_type == 4)\n",
    "gal_data = gal_data[type_3_4]\n",
    "full_g_data = full_g_data[type_3_4]\n",
    "full_r_data = full_r_data[type_3_4]\n",
    "gal_type = gal_type[type_3_4]\n",
    "\n",
    "# Zero out the gal_type column in the input data, so it can't be used to cheat.  :)\n",
    "gal_data['gal_type'][:] = 0\n",
    "\n",
    "# Split up the input into a training and validation set as you want.\n",
    "# When we run this, we will use the full data arrays as input, and some different (not provided!) data as validation.\n",
    "### EDIT THIS CODE...\n",
    "\n",
    "#generalize so you are able to choose various fractions of available data to run through neural net\n",
    "n=1000\n",
    "training_data = gal_data[:n]\n",
    "training_data_g = full_g_data[:n]\n",
    "training_data_r = full_r_data[:n]\n",
    "training_gal_type = gal_type[:n]\n",
    "validation_data = gal_data[n:]\n",
    "validation_data_g = full_g_data[n:]\n",
    "validation_data_r = full_r_data[n:]\n",
    "validation_gal_type = gal_type[n:]\n",
    "\n",
    "# Your function can ignore training_data_g and training_data_r if you don't want to use them.\n",
    "# But the function signature needs to take 4 values, since we will pass them to your function when we run this.\n",
    "result = train_galaxy_predictions_2(training_gal_type, training_data, training_data_g, training_data_r)\n",
    "predicted_gal_type = validate_galaxy_predictions_2(result, validation_data, validation_data_g, validation_data_r)\n",
    "\n",
    "#run predict function on the training set to see how well it does (see confusion matrix for training data set below)\n",
    "predict_train2 = validate_galaxy_predictions_2(result, training_data, training_data_g, training_data_r)\n",
    "\n",
    "print('Num correct = ',np.sum(predicted_gal_type == validation_gal_type))\n",
    "print('Num total = ',len(validation_gal_type))\n",
    "print('Fraction correct = ',np.sum(predicted_gal_type == validation_gal_type) / len(validation_gal_type))\n",
    "# Note: It may be helpful to construct a confusion matrix to better quantify how well you are doing here, rather\n",
    "# than just looking at the overal fraction correct.\n",
    "\n",
    "#create confusion matrix for test data set\n",
    "confusion_test = confusion_matrix(predicted_gal_type, validation_gal_type)\n",
    "\n",
    "#create confusion matrix for train data set\n",
    "training_gal_type[training_gal_type == 1] = 4\n",
    "training_gal_type[training_gal_type == 0] = 3\n",
    "confusion_train = confusion_matrix(predict_train2, training_gal_type)\n",
    "\n",
    "#print out confusion matrices\n",
    "print(\"Test is: \")\n",
    "print(confusion_test)\n",
    "print(\"Train is: \")\n",
    "print(confusion_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "Finally, in the third stage, we will try to classify all 4 types.\n",
    "\n",
    "You may find it simpler to do this in two steps.  First separate E and S0 galaxies (aka, types 1 and 2) from spiral galaxies (S1 and S2, aka types 3 and 4).  Then within these two groups classify E vs S0 and S1 vs S2.  This would allow you to use classifiers that are best done on binary classification.  \n",
    "\n",
    "Or you may want to try to use a classifier that can handle more than 2 types for the target and do the whole process at once.  Or you may find that some other combination of methods is appropriate.\n",
    "\n",
    "It should be noted that we don't expect you to be able to do a great job separating the E and S0 galaxies from each other.  So don't feel too bad if you aren't getting great results on that part of the classification.  However, we think you should be able to do a decent job separating the other types from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diana\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\diana\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.2265 - acc: 0.4410\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s 264us/step - loss: 0.9459 - acc: 0.6400\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s 312us/step - loss: 0.8473 - acc: 0.6690\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s 295us/step - loss: 0.8144 - acc: 0.6820\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s 310us/step - loss: 0.7764 - acc: 0.7130\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s 191us/step - loss: 0.7736 - acc: 0.7040\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s 268us/step - loss: 0.7498 - acc: 0.7010\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s 195us/step - loss: 0.7709 - acc: 0.7030\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s 269us/step - loss: 0.7379 - acc: 0.7180\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s 193us/step - loss: 0.7362 - acc: 0.7200\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s 272us/step - loss: 0.7449 - acc: 0.7170\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s 190us/step - loss: 0.7123 - acc: 0.7180\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 0s 261us/step - loss: 0.7192 - acc: 0.7210\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 0s 197us/step - loss: 0.7169 - acc: 0.7280\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 0s 250us/step - loss: 0.7178 - acc: 0.7230\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 0s 207us/step - loss: 0.7147 - acc: 0.7290\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 0s 241us/step - loss: 0.7154 - acc: 0.7160\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 0s 212us/step - loss: 0.6697 - acc: 0.7400\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 0s 235us/step - loss: 0.6913 - acc: 0.7280\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 0s 272us/step - loss: 0.7039 - acc: 0.7250\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 0s 241us/step - loss: 0.6952 - acc: 0.7370\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 0s 207us/step - loss: 0.6965 - acc: 0.7300\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 0s 193us/step - loss: 0.7003 - acc: 0.7370\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 0s 258us/step - loss: 0.6843 - acc: 0.7250\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 0s 200us/step - loss: 0.6971 - acc: 0.7290\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 0s 250us/step - loss: 0.6834 - acc: 0.7160\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 0s 222us/step - loss: 0.6904 - acc: 0.7400\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 0s 205us/step - loss: 0.6747 - acc: 0.7390\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 0s 272us/step - loss: 0.6769 - acc: 0.7360\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 0s 198us/step - loss: 0.6937 - acc: 0.7370\n",
      "Num correct =  729\n",
      "Num total =  1063\n",
      "Fraction correct =  0.6857949200376293\n",
      "Test is: \n",
      "[[236  29   4   2]\n",
      " [ 30 115  44   5]\n",
      " [  9  45  79  33]\n",
      " [  3  16 114 299]]\n",
      "Train is: \n",
      "[[138  22  14   4]\n",
      " [ 21 177  50   3]\n",
      " [  3  29  69  17]\n",
      " [  2   9  49 393]]\n"
     ]
    }
   ],
   "source": [
    "#import necessary modules and functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler as scale\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def train_galaxy_predictions_3(gal_type, data, data_g, data_r):\n",
    "    \"\"\"Do some kind of training to learn how to predict the galaxy type from the given training data.\n",
    "    \"\"\"\n",
    "    \n",
    "    #seed predictor data and response into dataframes for easier manipulation\n",
    "    predictor = pd.DataFrame(gal_type)    \n",
    "    df_data = pd.DataFrame(data)\n",
    "    \n",
    "    #delete unwanted columns, i.e. false predictors\n",
    "    df_data = df_data.drop(['id', 'gal_type'], axis= 1)\n",
    "    \n",
    "    #convert back from dataframes to arrays since that is what functions as input in keras\n",
    "    train = df_data.values\n",
    "    response = predictor.values\n",
    "    \n",
    "    #since this is a categorical classification problem, instead of assigning binary values, use built-in encoder to create\n",
    "    #the necessary one-hot encoded vectors for the 4 types of galaxies. in this case, the output is permutations of 1 and 0s\n",
    "    #for the number of possible response variables (classes), like [1,0,0,0], [0,1,0,0], etc.\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(response)\n",
    "    encoded_Y = encoder.transform(response)    \n",
    "    train_response = to_categorical(encoded_Y)\n",
    "    \n",
    "    #scale the predictor data accordingly.\n",
    "    #scale function applies a transformation that results in a standard normal distribution ~ Z(0,1)\n",
    "    sc = scale()\n",
    "    scale_train = sc.fit_transform(train)\n",
    "    \n",
    "    #build the tree with dropout layers to prevent overfitting\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(input_shape = (df_data.values.shape[1],), units= 16, kernel_initializer= 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dropout(0.3, noise_shape=None, seed=None))\n",
    "    classifier.add(Dense(units = 12, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dropout(0.3, noise_shape=None, seed=None))\n",
    "    classifier.add(Dense(units = 4, activation = 'softmax'))\n",
    "    \n",
    "    #compile tree with chosen optimizer and loss function\n",
    "    #categorical cross-entropy is chosen as this is a categorical classification problem\n",
    "    classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    #train neural net\n",
    "    classifier.fit(scale_train, train_response, batch_size = 6, epochs = 30)\n",
    "    \n",
    "    #return neural net as an object for use\n",
    "    return(classifier)\n",
    "\n",
    "def validate_galaxy_predictions_3(result, data, data_g, data_r):\n",
    "    \"\"\"Use the output from the above function to predict the galaxy type for some other validation data.\n",
    "    \"\"\"\n",
    "    \n",
    "    #use neural net that resulted from running train_galaxy_predictions_3 method\n",
    "    classifier = result\n",
    "    \n",
    "    #seed predictor data into dataframe for easier manipulation\n",
    "    df_data = pd.DataFrame(data)\n",
    "    \n",
    "    #delete unwanted columns, i.e. false predictors\n",
    "    df_data = df_data.drop(['id', 'gal_type'], axis= 1)\n",
    "    \n",
    "    #convert back from dataframes to arrays since that is what functions as input in keras\n",
    "    test = df_data.values\n",
    "    \n",
    "    #scale the predictor data accordingly.\n",
    "    #scale function applies a transformation that results in a standard normal distribution ~ Z(0,1)\n",
    "    sc = scale()\n",
    "    scale_test = sc.fit_transform(test)\n",
    "    \n",
    "    #predict which class the data is in\n",
    "    y_pred = classifier.predict_classes(scale_test)\n",
    "    \n",
    "    #assign proper values based on the result of the one-hot encoding\n",
    "    #the numbers 0,1,2,3 refer to the placement of the 1 in the vectors [1,0,0,0], [0,1,0,0], etc.\n",
    "    y_pred[y_pred == 3] = 4\n",
    "    y_pred[y_pred == 2] = 3\n",
    "    y_pred[y_pred == 1] = 2\n",
    "    y_pred[y_pred == 0] = 1\n",
    "    \n",
    "    #reformat\n",
    "    y_pred = y_pred.reshape((y_pred.shape[0],))\n",
    "    \n",
    "    #return predictor values\n",
    "    return(y_pred)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gal_file_name = 'training_galaxies.dat'\n",
    "full_g_file_name = 'training_galaxies_full_g.dat'\n",
    "full_r_file_name = 'training_galaxies_full_r.dat'\n",
    "\n",
    "gal_data = np.genfromtxt(gal_file_name, names=True, dtype=None)\n",
    "full_g_data = np.genfromtxt(full_g_file_name, names=True, dtype=None)\n",
    "full_r_data = np.genfromtxt(full_r_file_name, names=True, dtype=None)\n",
    "\n",
    "# Zero out the gal_type column in the input data, so it can't be used to cheat.  :)\n",
    "gal_type = gal_data['gal_type'].copy()\n",
    "gal_data['gal_type'][:] = 0\n",
    "\n",
    "# Split up the input into a training and validation set as you want.\n",
    "# When we run this, we will use the full data arrays as input, and some different (not provided!) data as validation.\n",
    "### EDIT THIS CODE...\n",
    "\n",
    "#generalize so you are able to choose various fractions of available data to run through neural net\n",
    "n=1000\n",
    "training_data = gal_data[:n]\n",
    "training_data_g = full_g_data[:n]\n",
    "training_data_r = full_r_data[:n]\n",
    "training_gal_type = gal_type[:n]\n",
    "validation_data = gal_data[n:]\n",
    "validation_data_g = full_g_data[n:]\n",
    "validation_data_r = full_r_data[n:]\n",
    "validation_gal_type = gal_type[n:]\n",
    "\n",
    "# Your function can ignore training_data_g and training_data_r if you don't want to use them.\n",
    "# But the function signature needs to take 4 values, since we will pass them to your function when we run this.\n",
    "result = train_galaxy_predictions_3(training_gal_type, training_data, training_data_g, training_data_r)\n",
    "predicted_gal_type = validate_galaxy_predictions_3(result, validation_data, validation_data_g, validation_data_r)\n",
    "\n",
    "#run predict function on the training set to see how well it does (see confusion matrix for training data set below)\n",
    "predict_train3 = validate_galaxy_predictions_3(result, training_data, training_data_g, training_data_r)\n",
    "\n",
    "print('Num correct = ',np.sum(predicted_gal_type == validation_gal_type))\n",
    "print('Num total = ',len(validation_gal_type))\n",
    "print('Fraction correct = ',np.sum(predicted_gal_type == validation_gal_type) / len(validation_gal_type))\n",
    "# Note: It may be helpful to construct a (4x4) confusion matrix to better quantify how well you are doing here, rather\n",
    "# than just looking at the overal fraction correct.\n",
    "\n",
    "#create confusion matrix for test data set\n",
    "confusion_test = confusion_matrix(predicted_gal_type, validation_gal_type)\n",
    "\n",
    "#create confusion matrix for train data set\n",
    "confusion_train = confusion_matrix(predict_train3, training_gal_type)\n",
    "\n",
    "#print confusion matrices\n",
    "print(\"Test is: \")\n",
    "print(confusion_test)\n",
    "print(\"Train is: \")\n",
    "print(confusion_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
